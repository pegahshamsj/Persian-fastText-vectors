# Persian-fastText-vectors
 Trained on 70G data
 <br/>
Here we trained fastText on a huge Persian dataset. The dataset is available [here](https://github.com/persiannlp/persian-raw-text).
<br/>
Generated vectors contain about 6742635 words. Each word is a vector of dimension 300.
<br/>
We used cbow model for computing word representations.
<br/>
1-5 grams of words used for training.
<br/>
You can also reduce the dimension if you needed.
<br/>
More instructions are provided [here](https://fasttext.cc/docs/en/support.html).
<br/>
* Pre-trained embedding vectors are available at [link](https://www.kaggle.com/datasets/pegahshams/persian-fasttext-vectors-trained-on-70g-data) cbow model.
<br/>
* Pre-trained model is available at [link](https://www.kaggle.com/datasets/pegahshams/persian-fasttext-model-trained-on-70g-data-cbow) cbow model.
