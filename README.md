# Persian-fastText-vectors
 Trained on 70G data
 <br/>
Here we trained fastText on a huge Persian dataset. The dataset is available [here](https://github.com/persiannlp/persian-raw-text).
<br/>
Generated vectors contain about 6742635 words. Each word is a vector of dimension 300.
<br/>
We used cbow model for computing word representations.
<br/>
1-5 grams of words used for training.
<br/>
You can also reduce the dimension if needed.
<br/>
More instructions are provided [here](https://fasttext.cc/docs/en/support.html).
<br/>
# Model Links
* Pre-trained embedding vectors (cbow) are available at [link](https://www.kaggle.com/datasets/pegahshams/persian-fasttext-vectors-trained-on-70g-data).
* Pre-trained model (cbow) is available at [link](https://www.kaggle.com/datasets/pegahshams/persian-fasttext-model-trained-on-70g-data-cbow).
* Pre-trained embedding vectors (skipgram) are available at [link](https://www.kaggle.com/datasets/pegahshams/farsi-fasttext-vectors-trainedon-70g-data-skipgram).
* Pre-trained model (skipgram) is available at [link](https://www.kaggle.com/datasets/pegahshams/persian-fasttext-model-trained-on-70g-data-sgram).
